{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "ECE-CS 5424 Assignment 2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mh6N3SPWUSMB"
      },
      "source": [
        "Advanced Machine Learning Assignment\n",
        "# Assignment 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIA3Hv8kUSMZ"
      },
      "source": [
        "In this assignment, **you need to complete the following two sectoins**:\n",
        "1. Logistic regression\n",
        "2. Regularization\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7vN9PE-ZFvg"
      },
      "source": [
        "# Section 0. Environment Set Up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Pja5zjFXwTp"
      },
      "source": [
        "Mount your google drive in google colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdVPU9EUXvij"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vg6RLG8AX73v"
      },
      "source": [
        "Append the directory to your python path using sys.\n",
        "\n",
        "**Please do modify the `customized_path_to_your_homework` to where you uploaded your homework in the Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwSblWSIX-qM"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "prefix = '/content/gdrive/My Drive/'\n",
        "# modify \"customized_path_to_your_homework\" here to where you uploaded your homework\n",
        "customized_path_to_your_homework = 'ECE 5424 ML/homework_spring/HW2'\n",
        "sys_path = os.path.join(prefix, customized_path_to_your_homework)\n",
        "sys.path.append(sys_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXkxh1F0USP7"
      },
      "source": [
        "# Section 1. Logistic Regression [25 pts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYOvYZ_4USP8"
      },
      "source": [
        "## Logistic Regression\n",
        "In this section, you need to implement logsitic regression to solve a binary classification problem. Let's first get our data ready:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3m-u3V1Xi6t"
      },
      "source": [
        "import os\n",
        "logistic_x_data_path = os.path.join(prefix, customized_path_to_your_homework, 'data/logistic_x_.txt')\n",
        "logistic_y_data_path = os.path.join(prefix, customized_path_to_your_homework, 'data/logistic_y_.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Op9aacdfWjBH"
      },
      "source": [
        "def feature_normalize(X):\n",
        "    \n",
        "    # FEATURENORMALIZE Normalizes the features in X \n",
        "    #   FEATURENORMALIZE(X) returns a normalized version of X where the mean value of each\n",
        "    #   feature is 0 and the standard deviation is 1. This is often a good preprocessing \n",
        "    #   step to do when working with learning algorithms.\n",
        "    \n",
        "    X_norm = X\n",
        "    mu     = 0\n",
        "    sigma  = 0\n",
        "\n",
        "    X_norm = X\n",
        "    mu     = np.mean(X, 0)\n",
        "    sigma  = np.std(X, 0)\n",
        "    X      = (X - mu) / sigma\n",
        "    X_norm = X;\n",
        "  \n",
        "    return X_norm, mu, sigma"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJJmpMWsUSP9"
      },
      "source": [
        "import numpy as np\n",
        "# Only use the first 70 samples for training (and validation),\n",
        "# and treat the rest of them as hold-out testing set.\n",
        "X = np.loadtxt(logistic_x_data_path) \n",
        "y = np.loadtxt(logistic_y_data_path).reshape(-1, 1) \n",
        "\n",
        "\n",
        "X, mu, std = feature_normalize(X)\n",
        "\n",
        "# Add a column of ones to X for the bias weight.\n",
        "m = len(X)\n",
        "X = np.concatenate((np.ones((m, 1)), X), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcrnuQikUSQE"
      },
      "source": [
        "Here, the input $x^{(i)}\\in\\mathbb{R^2}$ and $y^{(i)}\\in\\{-1, 1\\}$. Like we have mentioned, it is better to visualize the data first before you start working on it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWu_JnqjUSQF"
      },
      "source": [
        "# Plot the feature according to their class label.\n",
        "# Note that we exclude column 0, which is the colunm we padded with one in the previous block.\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(X[np.where(y==1), 1], X[np.where(y==1), 2], 'rx')\n",
        "plt.plot(X[np.where(y==-1), 1], X[np.where(y==-1), 2], 'bo')  \n",
        "plt.xlabel('x2')\n",
        "plt.ylabel('x1')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubw2Ad2MUSQM"
      },
      "source": [
        "In the following, you need to implement logistic regression. Recall that when $y^{(i)}\\in{-1,1}$, the objective function for binary logistic regression can be expressed as:\n",
        "\\begin{equation*}\n",
        "J(\\theta) = \\frac{1}{m}\\sum_{i=1}^{m}\\log{\\left(1+e^{-y^{(i)\\theta^Tx^{(i)}}}\\right)}=-\\frac{1}{m}\\sum_{i=1}^m\\log{\\left(h_{\\theta}(y^{(i)}x^{(i)})\\right)}\n",
        "\\end{equation*}\n",
        "where the hypothesis is the **sigmoid function**: \n",
        "\\begin{equation*}\n",
        "h_\\theta(y^{(i)}x^{(i)})=\\frac{1}{1+e^{-y^{(i)}\\theta^{T}x^{(i)}}}\n",
        "\\end{equation*}\n",
        "which we have seen in class (and assignment 0). Similar to the previous section, we can minimize the objective function $J(\\theta)$ using  batch gradient descent:\n",
        "\\begin{equation*}\n",
        "\\theta_j := \\theta_j - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}h_\\theta(-y^{(i)}x_j^{(i)})(-y^{(i)}x_j^{(i)})\n",
        "\\end{equation*}\n",
        "\n",
        "Now, your task is to complete the function `sigmoid`, `compute_cost`, `gradient_descent` for logistic regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xqd3w1jYUSQO"
      },
      "source": [
        "def sigmoid(z):\n",
        "    #####################################################################\n",
        "    # Instructions: Implement sigmoid function g                        #\n",
        "    #####################################################################\n",
        "    pass\n",
        "    #####################################################################\n",
        "    #                       END OF YOUR CODE                            #\n",
        "    #####################################################################\n",
        "    return g\n",
        "\n",
        "def compute_cost(X, y, theta):\n",
        "    \n",
        "    # You need to return the following variables correctly \n",
        "    J = 0;\n",
        "    #####################################################################\n",
        "    # Instructions: Implement the objective function J(theta)           #\n",
        "    #####################################################################\n",
        "    pass\n",
        "    #####################################################################\n",
        "    #                       END OF YOUR CODE                            #\n",
        "    #####################################################################\n",
        "    return J\n",
        "\n",
        "def compute_gradient(X, y, theta):\n",
        "    #####################################################################\n",
        "    # Instructions: Implement gradient function gradient_               #\n",
        "    #####################################################################\n",
        "    pass\n",
        "    #####################################################################\n",
        "    #                       END OF YOUR CODE                            #\n",
        "    #####################################################################\n",
        "    return gradient_\n",
        "\n",
        "\n",
        "def gradient_descent_logistic(X, y, theta, alpha, num_iters):\n",
        "    m = len(y)\n",
        "    J_history = []\n",
        "    for iter in range(num_iters):\n",
        "\n",
        "        \n",
        "        #####################################################################\n",
        "        # Instructions: Perform a single gradient step on the parameter     #\n",
        "        #               vector theta using the implemented compute_gradient #\n",
        "        #                                                                   #      \n",
        "        # Hint: While debugging, it can be useful to print out the values   #\n",
        "        #       of the cost function (compute_cost) and gradient here.      # \n",
        "        #####################################################################\n",
        "        pass\n",
        "        #####################################################################\n",
        "        #                       END OF YOUR CODE                            #\n",
        "        #####################################################################\n",
        "    \n",
        "\n",
        "        # Save the cost J in every iteration \n",
        "        J = compute_cost(X, y, theta)\n",
        "        print(J)\n",
        "        J_history.append(J)\n",
        "    \n",
        "    return theta, J_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbqxwYXNUSQR"
      },
      "source": [
        "Now, fit your model, and see if it is learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lZwfDtsUSQT"
      },
      "source": [
        "# Train your model.\n",
        "theta = np.zeros((X.shape[1], 1))\n",
        "alpha = 0.1;\n",
        "num_iters = 400;\n",
        "theta, J_history = gradient_descent_logistic(X, y, theta, alpha, num_iters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBz-vuy6USQX"
      },
      "source": [
        "Again, plot and check to see if the model is converging."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tI7-ve1lUSQY"
      },
      "source": [
        "plt.plot(list(range(0, len(J_history))), J_history, '-b')  \n",
        "plt.xlabel('Number of iterations')\n",
        "plt.ylabel('Cost J')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJ7vClqtUSQf"
      },
      "source": [
        "## Decision Boundary\n",
        "In addition to checking convergence graph and accuracy, we can also plot out the decision boundary to see what does the model actually learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDhTGlzFUSQf"
      },
      "source": [
        "# Plot the feature according to their class label.\n",
        "# Note that we exclude column 0, which is the colunm we padded with one in the previous block.\n",
        "plt.plot(X[np.where(y==1), 1], X[np.where(y==1), 2], 'rx')\n",
        "plt.plot(X[np.where(y==-1), 1], X[np.where(y==-1), 2], 'bo')\n",
        "\n",
        "#####################################################################\n",
        "# Instructions: Plot out the decision boundary.                     #\n",
        "# Hint: To plot the boundary, which is a straight line in our case, #\n",
        "#       you need to find the two ends of the line, and plot it with #\n",
        "#       plt.plot(). Note that the decision boundary is the line that#\n",
        "#       y = 0.                                                      # \n",
        "#####################################################################\n",
        "pass\n",
        "#####################################################################\n",
        "#                       END OF YOUR CODE                            #\n",
        "#####################################################################\n",
        "\n",
        "plt.xlabel('x1')\n",
        "plt.ylabel('x2')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shWKz8-hUSQi"
      },
      "source": [
        "# Section 2. Regularization [25 pts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6u2mu0-USQi"
      },
      "source": [
        "In this section, you need to incorporate L2 regularization into your logistic regression. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qs9SvcGsUSQj"
      },
      "source": [
        "## L2 Regularization\n",
        "Overfitting is a notorious problem in the world of machine learning. One simple way to counter this issue is to put constraints on your model weights $\\theta$, as we have discussed in class. In this section, you need to modify the the objective function to impose L2 regularization on the logistic regression:\n",
        "\\begin{equation*}\n",
        "    J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^m\\log{\\left(h_{\\theta}(y^{(i)}x^{(i)})\\right)} + \\lambda\\vert\\vert\\theta\\vert\\vert_2^2\n",
        "\\end{equation*}\n",
        "Derive the gradient for this new objective to incorporate it into your logistic regression model.\n",
        "\n",
        "To make things much structural, we now put everything together into a class. Please use the class template below to implement your logistic regression. Note that you can add your own class methods if needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8R56endnUSQp"
      },
      "source": [
        "class LogisticRegression(object):\n",
        "    \n",
        "    def __init__(self, alpha=0.1, lamb=0.1, regularization=None):\n",
        "        # setting the class attribute.\n",
        "        self.alpha = alpha                   # Set up your learning rate alpha.\n",
        "        self.lamb = lamb                     # Strength of regularization.\n",
        "        self.regularization = regularization \n",
        "        assert regularization == 'l2' or regularization == None # we only consider these two cases\n",
        "    \n",
        "    def _compute_cost(self, X, y):\n",
        "        #####################################################################\n",
        "        # Instructions: Compute the cost function here.                     #\n",
        "        #               You need to handle both the cases with, and without #\n",
        "        #               regularization here.                                #\n",
        "        #####################################################################\n",
        "        pass\n",
        "        #####################################################################\n",
        "        #                       END OF YOUR CODE                            #\n",
        "        #####################################################################\n",
        "        return J\n",
        "        \n",
        "    def _compute_gradient(self, X, y):\n",
        "        #####################################################################\n",
        "        # Instructions: Compute the gradient here.                          #\n",
        "        #               You need to handle both the cases with, and without #\n",
        "        #               regularization here.                                #\n",
        "        #####################################################################\n",
        "        pass\n",
        "        #####################################################################\n",
        "        #                       END OF YOUR CODE                            #\n",
        "        #####################################################################\n",
        "        return gradient\n",
        "\n",
        "    def fit(self, X, y, num_iter=5):\n",
        "        self.theta = np.zeros((X.shape[1], 1))\n",
        "        m = len(y)\n",
        "        J_history = []\n",
        "        #####################################################################\n",
        "        # Instructions: Run the gradient decsent here.                      #\n",
        "        #####################################################################\n",
        "        pass\n",
        "        #####################################################################\n",
        "        #                       END OF YOUR CODE                            #\n",
        "        #####################################################################\n",
        "        return J_history\n",
        "    \n",
        "    def predict(self, X):\n",
        "        #####################################################################\n",
        "        # Instructions: Use your hypothese to make predictions.             #\n",
        "        #####################################################################\n",
        "        pass\n",
        "        #####################################################################\n",
        "        #                       END OF YOUR CODE                            #\n",
        "        #####################################################################\n",
        "        return y_hat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvOuwb--USQr"
      },
      "source": [
        "Load the wine datasets, in which $x_j\\in\\mathbb{R}^{12}$ is different attribute for alcohol, and $y\\in\\{-1,1\\}$ is that class label (red or white wine)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcJhvSKZUSQs"
      },
      "source": [
        "# Load dataset\n",
        "import numpy as np\n",
        "X_train = np.loadtxt(os.path.join(prefix, customized_path_to_your_homework, 'data/wine_train_X.txt'))\n",
        "y_train = np.loadtxt(os.path.join(prefix, customized_path_to_your_homework, 'data/wine_train_y.txt')).reshape(-1, 1)\n",
        "X_test = np.loadtxt(os.path.join(prefix, customized_path_to_your_homework, 'data/wine_test_X.txt'))\n",
        "y_test = np.loadtxt(os.path.join(prefix, customized_path_to_your_homework, 'data/wine_test_y.txt')).reshape(-1, 1)\n",
        "\n",
        "\n",
        "X_train = np.concatenate((np.ones((X_train.shape[0], 1)), X_train), axis=1)\n",
        "X_test = np.concatenate((np.ones((X_test.shape[0], 1)), X_test), axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrVpJHEaUSQx"
      },
      "source": [
        "Now, let's train two different logistic regression models: one with, and one without regularization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3L4_u_m5USQy"
      },
      "source": [
        "log_reg = LogisticRegression(alpha=0.1) # Without regularization\n",
        "log_reg_l2 = LogisticRegression(alpha=0.1, lamb=1.0, regularization='l2') # Without regularization\n",
        "\n",
        "J_history = log_reg.fit(X_train, y_train, num_iter=500)\n",
        "J_history_l2 = log_reg_l2.fit(X_train, y_train, num_iter=500)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LbnavioUSQ2"
      },
      "source": [
        "Next, we evaluate the accuracy for each method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUPmeq4BUSQ3"
      },
      "source": [
        "def evaluate_accuracy(X, y, model):\n",
        "    y_pred = model.predict(X)\n",
        "    y_pred[y_pred > 0.5] = 1\n",
        "    y_pred[y_pred <= 0.5] = -1\n",
        "    return np.mean(y_pred == y)\n",
        "\n",
        "print(\"Accuracy on training set: \", evaluate_accuracy(X_train, y_train, log_reg))\n",
        "print(\"Accuracy on testing set: \", evaluate_accuracy(X_test, y_test, log_reg))\n",
        "print(\"Accuracy w/ L2 training set: \", evaluate_accuracy(X_train, y_train, log_reg_l2))\n",
        "print(\"Accuracy w/ L2 testing set: \", evaluate_accuracy(X_test, y_test, log_reg_l2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHzJ9-fiUSQ6"
      },
      "source": [
        "To see the effect of regularization on $\\theta$, we can plot out each $\\theta_j$ under different $\\lambda$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruUGI7ZbUSQ6"
      },
      "source": [
        "def plot_theta(theta, lamb):\n",
        "    \"\"\"\n",
        "    Helper function for plotting out the value of theta with respect to different lambda.\n",
        "    theta  (list): list of theta under different lambda.\n",
        "    lambda (list): list of lambda values you tried.\n",
        "    \"\"\"\n",
        "    plt.hlines(y=0, xmin=0, xmax=np.max(lamb), color='red', linewidth = 2, linestyle = '--')\n",
        "    for i in range(theta.shape[1]):\n",
        "        plt.plot(lamb, theta[:,i])\n",
        "    plt.ylabel('theta')\n",
        "    plt.xlabel('lambda')\n",
        "    plt.xscale('log')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrXZdXtLUSQ_"
      },
      "source": [
        "lamb = [0.1, 1, 10, 100, 1000]\n",
        "theta = []\n",
        "\n",
        "#####################################################################\n",
        "# Instructions: For each value in lamb, try a model for it, and     #\n",
        "#               append the trained weights into the theta           #\n",
        "#####################################################################\n",
        "pass\n",
        "#####################################################################\n",
        "#                       END OF YOUR CODE                            #\n",
        "#####################################################################\n",
        "\n",
        "plot_theta(np.array(theta), lamb)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}